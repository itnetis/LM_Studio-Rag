{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec294d3f-5d23-4299-ac31-a66b2b5e0d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import hashlib\n",
    "import pickle\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279e425-7eb2-4954-8064-43d7cb8688b2",
   "metadata": {},
   "source": [
    "# LM Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "996e31c0-5fe6-41a2-b5dd-3db85b8f5830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Found 2 PDF files.\n",
      "\n",
      "üìö Processing batch 1/1...\n",
      "‚úÖ Using cached chunks from Test_data_chun\\chunks_batch_1.pkl\n",
      "‚úÇÔ∏è Total chunks: 56\n",
      "üíæ Saved vectorstore part 1 at Test_data_Store\\vectorstore_part_1\n",
      "\n",
      "üîÑ Merging all vectorstore parts...\n",
      "‚úÖ Merged part 1\n",
      "\n",
      "üéâ Final vectorstore saved to 'Test_data_merged'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import hashlib\n",
    "import pickle\n",
    "import requests  # <-- you forgot this\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "load_dotenv()\n",
    "\n",
    "# === Config ===\n",
    "PDF_FOLDER = \"Data\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 150\n",
    "PDF_BATCH_SIZE = 10\n",
    "VECTORSTORE_DIR = \"Test_data_Store\"\n",
    "MERGED_VECTORSTORE_PATH = \"Test_data_merged\"\n",
    "CHUNK_SAVE_DIR = \"Test_data_chun\"\n",
    "EMBED_MODEL = \"text-embedding-nomic-embed-text-v1.5\"   # LM Studio model name\n",
    "LMSTUDIO_URL = \"http://localhost:1234/v1/embeddings\"\n",
    "\n",
    "os.makedirs(VECTORSTORE_DIR, exist_ok=True)\n",
    "os.makedirs(CHUNK_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# === Utility: List PDFs ===\n",
    "def list_pdf_files(data_dir):\n",
    "    return [\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in os.walk(data_dir)\n",
    "        for file in files if file.endswith(\".pdf\")\n",
    "    ]\n",
    "\n",
    "\n",
    "# === Utility: Split list into batches ===\n",
    "def split_list(lst, chunk_size):\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i:i + chunk_size]\n",
    "\n",
    "\n",
    "# === Utility: Check if pickle is still valid ===\n",
    "def is_pickle_up_to_date(pdf_paths, pkl_path):\n",
    "    if not os.path.exists(pkl_path):\n",
    "        return False\n",
    "    pkl_mtime = os.path.getmtime(pkl_path)\n",
    "    return all(os.path.getmtime(pdf) <= pkl_mtime for pdf in pdf_paths)\n",
    "\n",
    "\n",
    "# === Step 1: Load PDFs ===\n",
    "def load_pdf_batch(pdf_batch):\n",
    "    all_docs = []\n",
    "    for pdf in pdf_batch:\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(pdf)\n",
    "            pages = loader.load()\n",
    "            for page in pages:\n",
    "                page.metadata[\"source\"] = os.path.basename(pdf)\n",
    "            all_docs.extend(pages)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {pdf}: {e}\")\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "# === Step 2: Chunk documents and save as .pkl ===\n",
    "def chunk_documents(docs, batch_index=None, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        add_start_index=True,\n",
    "        keep_separator=True,\n",
    "    )\n",
    "\n",
    "    print(\"‚úÇÔ∏è Chunking documents...\")\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    # Add chunk_id using hash\n",
    "    for chunk in chunks:\n",
    "        content_hash = hashlib.md5(chunk.page_content.encode()).hexdigest()[:8]\n",
    "        chunk.metadata[\"chunk_id\"] = f\"{chunk.metadata['source']}_{chunk.metadata.get('page', 0)}_{content_hash}\"\n",
    "\n",
    "    # Save to .pkl\n",
    "    if batch_index is not None:\n",
    "        chunk_file = os.path.join(CHUNK_SAVE_DIR, f\"chunks_batch_{batch_index + 1}.pkl\")\n",
    "        with open(chunk_file, \"wb\") as f:\n",
    "            pickle.dump(chunks, f)\n",
    "        print(f\"üíæ Saved {len(chunks)} chunks to {chunk_file}\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# === LM Studio Embeddings Wrapper ===\n",
    "\n",
    "class LMStudioEmbeddings(Embeddings):\n",
    "    def __init__(self, model=EMBED_MODEL, base_url=LMSTUDIO_URL):\n",
    "        self.model = model\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def embed_query(self, text: str):\n",
    "        return self._embed([text])[0]\n",
    "\n",
    "    def embed_documents(self, texts: list[str]):\n",
    "        return self._embed(texts)\n",
    "\n",
    "    def _embed(self, texts: list[str]):\n",
    "        response = requests.post(\n",
    "            self.base_url,\n",
    "            json={\"model\": self.model, \"input\": texts},\n",
    "            headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if \"data\" not in data or len(data[\"data\"]) == 0:\n",
    "            raise ValueError(f\"‚ùå No embeddings returned. Response: {data}\")\n",
    "\n",
    "        return [item[\"embedding\"] for item in data[\"data\"]]\n",
    "\n",
    "\n",
    "\n",
    "# === Step 3: Create vectorstore ===\n",
    "# def create_vectorstore(chunks, embeddings):\n",
    "#     sample_vec = embeddings.embed_query(\"test\")\n",
    "#     index = faiss.IndexFlatL2(len(sample_vec))\n",
    "#     vectorstore = FAISS(\n",
    "#         embedding=embeddings,\n",
    "#         index=index,\n",
    "#         docstore=InMemoryDocstore(),\n",
    "#         index_to_docstore_id={}\n",
    "#     )\n",
    "#     vectorstore.add_documents(chunks)\n",
    "#     return vectorstore\n",
    "# === Step 3: Create vectorstore ===\n",
    "def create_vectorstore(chunks, embeddings):\n",
    "    if not chunks:\n",
    "        raise ValueError(\"‚ùå No chunks provided to create_vectorstore\")\n",
    "\n",
    "    test_vec = embeddings.embed_query(\"test\")\n",
    "    if not test_vec:\n",
    "        raise ValueError(\"‚ùå Embedding model returned empty vector for test input\")\n",
    "\n",
    "    return FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Save/load vectorstore ===\n",
    "def save_vectorstore(vs, path):\n",
    "    vs.save_local(path)\n",
    "\n",
    "def load_vectorstore(path, embeddings):\n",
    "    return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "\n",
    "# === Main ===\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_files = list_pdf_files(PDF_FOLDER)\n",
    "    print(f\"üìÑ Found {len(pdf_files)} PDF files.\")\n",
    "\n",
    "    embeddings = LMStudioEmbeddings(model=EMBED_MODEL, base_url=LMSTUDIO_URL)\n",
    "    pdf_batches = list(split_list(pdf_files, PDF_BATCH_SIZE))\n",
    "\n",
    "    for i, batch in enumerate(pdf_batches):\n",
    "        print(f\"\\nüìö Processing batch {i+1}/{len(pdf_batches)}...\")\n",
    "        chunk_file = os.path.join(CHUNK_SAVE_DIR, f\"chunks_batch_{i + 1}.pkl\")\n",
    "\n",
    "        if is_pickle_up_to_date(batch, chunk_file):\n",
    "            print(f\"‚úÖ Using cached chunks from {chunk_file}\")\n",
    "            with open(chunk_file, \"rb\") as f:\n",
    "                chunks = pickle.load(f)\n",
    "        else:\n",
    "            print(\"üîÅ PDFs modified or chunks not found. Re-processing...\")\n",
    "            docs = load_pdf_batch(batch)\n",
    "            print(f\"‚úÖ Loaded {len(docs)} pages\")\n",
    "            chunks = chunk_documents(docs, batch_index=i)\n",
    "\n",
    "        print(f\"‚úÇÔ∏è Total chunks: {len(chunks)}\")\n",
    "        vs = create_vectorstore(chunks, embeddings)\n",
    "        part_path = os.path.join(VECTORSTORE_DIR, f\"vectorstore_part_{i + 1}\")\n",
    "        save_vectorstore(vs, part_path)\n",
    "        print(f\"üíæ Saved vectorstore part {i + 1} at {part_path}\")\n",
    "\n",
    "    # === Merge parts ===\n",
    "    print(\"\\nüîÑ Merging all vectorstore parts...\")\n",
    "    merged_vs = None\n",
    "    for i in range(1, len(pdf_batches) + 1):\n",
    "        part_path = os.path.join(VECTORSTORE_DIR, f\"vectorstore_part_{i}\")\n",
    "        if not os.path.exists(part_path):\n",
    "            print(f\"‚ö†Ô∏è Missing part: {part_path}\")\n",
    "            continue\n",
    "        part_vs = load_vectorstore(part_path, embeddings)\n",
    "        if merged_vs is None:\n",
    "            merged_vs = part_vs\n",
    "        else:\n",
    "            merged_vs.merge_from(part_vs)\n",
    "        print(f\"‚úÖ Merged part {i}\")\n",
    "\n",
    "    if merged_vs:\n",
    "        merged_vs.save_local(MERGED_VECTORSTORE_PATH)\n",
    "        print(f\"\\nüéâ Final vectorstore saved to '{MERGED_VECTORSTORE_PATH}'\")\n",
    "    else:\n",
    "        print(\"‚ùå No vectorstores were merged.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b342fde-7469-4f28-b522-826656f98be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ae3786b-4573-4fd9-9c17-624d5f45b230",
   "metadata": {},
   "source": [
    "# Chat Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e677f5c-63da-421f-bdad-bebb684ca6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LMStudioLLM:\n",
    "    def __init__(self, model=\"humanizerai\", base_url=\"http://localhost:1234/v1/chat/completions\"):\n",
    "        self.model = model\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def generate(self, prompt, temperature=0.7, max_tokens=512):\n",
    "        \"\"\"Send a chat-style completion request to LM Studio\"\"\"\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "        response = requests.post(\n",
    "            self.base_url,\n",
    "            json=payload,\n",
    "            headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if \"choices\" not in data or not data[\"choices\"]:\n",
    "            raise ValueError(f\"‚ùå No response from LM Studio. Response: {data}\")\n",
    "\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b426be1d-767d-4faf-85c3-d76db564ea0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a58ebde-28d1-4b67-ac9a-561d67c16905",
   "metadata": {},
   "source": [
    "# Eg RAG Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b957adf-517d-4009-91e9-ffeff69b8869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ The passage mentions several books and articles that discuss AI development, including Carl Jung's \"Types of Personality\" test and Isabel Briggs Myers' \"Personality Type Questionnaire.\" However, there is no mention of \"temp.pdf.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load your merged vectorstore\n",
    "# ‚úÖ Load merged vectorstore correctly\n",
    "embeddings = LMStudioEmbeddings(model=EMBED_MODEL, base_url=LMSTUDIO_URL)\n",
    "vs = FAISS.load_local(\n",
    "    \"Test_data_merged\",      # folder path\n",
    "    embeddings,              # your LMStudioEmbeddings instance\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap retriever\n",
    "retriever = vs.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# LM Studio LLM\n",
    "llm = LMStudioLLM(model=\"humanizerai\")\n",
    "\n",
    "# Ask a question\n",
    "query = \"What is written in temp.pdf about AI development?\"\n",
    "context_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Build context\n",
    "context_text = \"\\n\\n\".join([d.page_content for d in context_docs])\n",
    "final_prompt = f\"Answer the question based on the context below:\\n\\n{context_text}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "answer = llm.generate(final_prompt)\n",
    "print(\"ü§ñ\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e409d-bbcd-4cdd-802a-b619c1017390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
